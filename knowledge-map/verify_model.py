#!/usr/bin/env python3
"""
éªŒè¯æœ¬åœ°BGEæ¨¡å‹
"""
import os
from pathlib import Path

def check_model_files():
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´"""
    model_path = Path("./data/models/bge-base-zh-v1.5")
    
    if not model_path.exists():
        print("âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹")
        print("   å‚è€ƒ MODEL_DOWNLOAD.md æ–‡ä»¶")
        return False
    
    required_files = [
        "config.json",
        "pytorch_model.bin", 
        "tokenizer.json",
        "tokenizer_config.json",
        "vocab.txt"
    ]
    
    print(f"ğŸ“ æ£€æŸ¥æ¨¡å‹ç›®å½•: {model_path.absolute()}")
    
    missing_files = []
    for file_name in required_files:
        file_path = model_path / file_name
        if file_path.exists():
            size_mb = file_path.stat().st_size / (1024 * 1024)
            print(f"âœ… {file_name} ({size_mb:.1f} MB)")
        else:
            missing_files.append(file_name)
            print(f"âŒ {file_name} (ç¼ºå¤±)")
    
    if missing_files:
        print(f"\nâŒ ç¼ºå°‘å¿…éœ€æ–‡ä»¶: {missing_files}")
        print("è¯·ä¸‹è½½æ‰€æœ‰å¿…éœ€æ–‡ä»¶åˆ° data/models/bge-base-zh-v1.5/ ç›®å½•")
        return False
    else:
        print("\nâœ… æ‰€æœ‰æ¨¡å‹æ–‡ä»¶å®Œæ•´")
        return True

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    try:
        print("\nğŸ”„ æµ‹è¯•æ¨¡å‹åŠ è½½...")
        from sentence_transformers import SentenceTransformer
        
        model_path = os.path.abspath("./data/models/bge-base-zh-v1.5")
        print(f"   æ¨¡å‹ç»å¯¹è·¯å¾„: {model_path}")
        
        # å°è¯•ä¸åŒçš„åŠ è½½æ–¹å¼
        try:
            # æ–¹å¼1ï¼šä½¿ç”¨ç»å¯¹è·¯å¾„
            model = SentenceTransformer(model_path)
        except:
            try:
                # æ–¹å¼2ï¼šä½¿ç”¨ trust_remote_code
                model = SentenceTransformer(model_path, trust_remote_code=True)
            except:
                # æ–¹å¼3ï¼šç›´æ¥ä½¿ç”¨transformersåº“
                from transformers import AutoTokenizer, AutoModel
                tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                model_hf = AutoModel.from_pretrained(model_path, trust_remote_code=True)
                
                # ç®€å•æµ‹è¯•
                inputs = tokenizer("æµ‹è¯•æ–‡æœ¬", return_tensors="pt")
                outputs = model_hf(**inputs)
                print(f"âœ… ä½¿ç”¨transformersåŠ è½½æˆåŠŸï¼")
                print(f"   è¾“å‡ºç»´åº¦: {outputs.last_hidden_state.shape[-1]}")
                return True
        
        # æµ‹è¯•ç¼–ç 
        test_texts = ['è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•', 'AI Agentæµ‹è¯•', 'äººå·¥æ™ºèƒ½']
        embeddings = model.encode(test_texts)
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print(f"   å‘é‡ç»´åº¦: {embeddings.shape[1]}")
        print(f"   æµ‹è¯•æ–‡æœ¬æ•°: {len(test_texts)}")
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ BGEæ¨¡å‹éªŒè¯å·¥å…·")
    print("=" * 40)
    
    # æ£€æŸ¥æ–‡ä»¶
    if not check_model_files():
        exit(1)
    
    # æµ‹è¯•åŠ è½½
    if not test_model_loading():
        exit(1)
        
    print("\nğŸ‰ BGEæ¨¡å‹éªŒè¯é€šè¿‡ï¼å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")