"""
å…ˆè¿›çš„å¼€æºembeddingæ¨¡å‹æä¾›è€…
æ”¯æŒå½“å‰æœ€ä½³çš„å¼€æºembeddingæ¨¡å‹ï¼Œä¸ºAI Agentæä¾›é«˜è´¨é‡çš„å‘é‡åŒ–æœåŠ¡
"""
import os
import torch
import numpy as np
from typing import List, Union, Dict, Any, Optional
from loguru import logger
import warnings
warnings.filterwarnings("ignore")

from interfaces.embedding_provider import EmbeddingProviderInterface


class AdvancedEmbeddingProvider(EmbeddingProviderInterface):
    """ä½¿ç”¨æœ€å…ˆè¿›å¼€æºæ¨¡å‹çš„embeddingæœåŠ¡"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–embeddingæœåŠ¡"""
        self.config = config or {}
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.dimension = 768  # é»˜è®¤ç»´åº¦
        self.model_name = None
        self.is_initialized = False
        self.batch_size = self.config.get('batch_size', 32)
        
        # å½“å‰æœ€å…ˆè¿›çš„å¼€æºembeddingæ¨¡å‹ï¼ˆ2024å¹´ï¼‰
        # æ”¯æŒæœ¬åœ°è·¯å¾„å’Œåœ¨çº¿æ¨¡å‹
        self.model_configs = [
            # æœ¬åœ°æ¨¡å‹ï¼ˆä¼˜å…ˆï¼‰
            {
                'name': './data/models/bge-large-zh-v1.5',
                'description': 'BGEå¤§å‹ä¸­æ–‡æ¨¡å‹(1024ç»´) - æœ¬åœ°ç‰ˆ',
                'dimension': 1024,
                'languages': ['zh', 'en'],
                'quality': 'excellent',
                'local': True
            },
            {
                'name': './data/models/bge-base-zh-v1.5',
                'description': 'BGEåŸºç¡€ä¸­æ–‡æ¨¡å‹(768ç»´) - æœ¬åœ°ç‰ˆ',
                'dimension': 768,
                'languages': ['zh', 'en'],
                'quality': 'very_good',
                'local': True
            },
            {
                'name': './data/models/all-mpnet-base-v2',
                'description': 'MPNetå…¨èƒ½æ¨¡å‹(768ç»´) - æœ¬åœ°ç‰ˆ',
                'dimension': 768,
                'languages': ['en'],
                'quality': 'good',
                'local': True
            },
            # åœ¨çº¿æ¨¡å‹ï¼ˆå¤‡ç”¨ï¼‰
            {
                'name': 'BAAI/bge-large-zh-v1.5',
                'description': 'BGEå¤§å‹ä¸­æ–‡æ¨¡å‹(1024ç»´) - åœ¨çº¿ç‰ˆ',
                'dimension': 1024,
                'languages': ['zh', 'en'],
                'quality': 'excellent',
                'local': False
            },
            {
                'name': 'BAAI/bge-base-zh-v1.5',
                'description': 'BGEåŸºç¡€ä¸­æ–‡æ¨¡å‹(768ç»´) - åœ¨çº¿ç‰ˆ',
                'dimension': 768,
                'languages': ['zh', 'en'],
                'quality': 'very_good',
                'local': False
            },
            {
                'name': 'sentence-transformers/all-mpnet-base-v2',
                'description': 'MPNetå…¨èƒ½æ¨¡å‹(768ç»´) - åœ¨çº¿ç‰ˆ',
                'dimension': 768,
                'languages': ['en'],
                'quality': 'good',
                'local': False
            }
        ]
        
        logger.info(f"åˆå§‹åŒ–å…ˆè¿›embeddingæœåŠ¡ï¼Œè®¾å¤‡: {self.device}")
        self._load_best_available_model()

    def _load_best_available_model(self):
        """è‡ªåŠ¨åŠ è½½æœ€ä½³å¯ç”¨æ¨¡å‹ï¼Œä¼˜å…ˆæœ¬åœ°æ¨¡å‹"""
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰é…ç½®çš„æœ¬åœ°æ¨¡å‹
        local_models = self.config.get('local_models', {})
        prefer_local = self.config.get('prefer_local', True)
        
        if prefer_local and local_models:
            logger.info("ğŸ” æ£€æŸ¥æœ¬åœ°æ¨¡å‹...")
            for model_key, model_path in local_models.items():
                if self._check_local_model_exists(model_path):
                    logger.info(f"âœ… å‘ç°æœ¬åœ°æ¨¡å‹: {model_key} -> {model_path}")
                    if self._try_load_model(model_path):
                        self.model_name = model_path
                        logger.success(f"ğŸ‰ æˆåŠŸåŠ è½½æœ¬åœ°æ¨¡å‹: {model_key}")
                        return
        
        # 2. ä¼˜å…ˆä½¿ç”¨é…ç½®æŒ‡å®šçš„æ¨¡å‹
        specified_model = self.config.get('model_name')
        if specified_model:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
            if os.path.exists(specified_model):
                logger.info(f"å°è¯•åŠ è½½æœ¬åœ°æŒ‡å®šæ¨¡å‹: {specified_model}")
                if self._try_load_model(specified_model):
                    self.model_name = specified_model
                    logger.success(f"âœ… æˆåŠŸåŠ è½½æœ¬åœ°æŒ‡å®šæ¨¡å‹")
                    return
            else:
                logger.info(f"å°è¯•åœ¨çº¿ä¸‹è½½æŒ‡å®šæ¨¡å‹: {specified_model}")
                if self._try_load_model(specified_model):
                    self.model_name = specified_model
                    logger.success(f"âœ… æˆåŠŸä¸‹è½½å¹¶åŠ è½½æŒ‡å®šæ¨¡å‹")
                    return
            logger.warning(f"æŒ‡å®šæ¨¡å‹ {specified_model} åŠ è½½å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ¨¡å‹")
        
        # 3. æŒ‰è´¨é‡é¡ºåºå°è¯•åœ¨çº¿ä¸‹è½½æ¨¡å‹
        logger.info("ğŸŒ å°è¯•åœ¨çº¿ä¸‹è½½é«˜çº§æ¨¡å‹...")
        for model_config in self.model_configs:
            model_name = model_config['name']
            logger.info(f"å°è¯•åŠ è½½ {model_config['description']}")
            
            if self._try_load_model(model_name, model_config):
                self.model_name = model_name
                self.dimension = model_config['dimension']
                logger.success(f"âœ… æˆåŠŸåŠ è½½ {model_config['description']}")
                logger.info(f"æ¨¡å‹ç»´åº¦: {self.dimension}, è¯­è¨€æ”¯æŒ: {model_config['languages']}")
                return
        
        # 4. å¦‚æœéƒ½å¤±è´¥äº†ï¼Œä½¿ç”¨ç®€å•çš„fallbackæ¨¡å‹
        if self.config.get('enable_fallback', True):
            logger.warning("âš ï¸ é«˜çº§æ¨¡å‹æ— æ³•åŠ è½½ï¼Œä½¿ç”¨ç¦»çº¿fallbackæ¨¡å‹")
            if self._load_fallback_model():
                logger.info("âœ… æˆåŠŸåŠ è½½ç¦»çº¿fallback embeddingæ¨¡å‹")
                return
        
        logger.error("âŒ æ‰€æœ‰æ¨¡å‹éƒ½æ— æ³•åŠ è½½ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ¨¡å‹å®‰è£…")
        logger.info("ğŸ’¡ å»ºè®®æ“ä½œ:")
        logger.info("   1. ä½¿ç”¨æä¾›çš„ä¸‹è½½è„šæœ¬:")
        logger.info("      Windows: powershell -ExecutionPolicy Bypass -File download_bge_model.ps1")
        logger.info("      Linux/Mac: ./download_bge_model.sh")
        logger.info("   2. æˆ–æ‰‹åŠ¨ä¸‹è½½ï¼Œå‚è€ƒ MANUAL_DOWNLOAD.md")
        logger.info("   3. æˆ–ä¸´æ—¶ä½¿ç”¨TF-IDFæ¨¡å‹è¿›è¡Œæµ‹è¯•")
        raise RuntimeError("æ— æ³•åŠ è½½ä»»ä½•embeddingæ¨¡å‹")

    def _check_local_model_exists(self, model_path: str) -> bool:
        """æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´"""
        if not os.path.exists(model_path):
            return False
        
        # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
        required_files = [
            'config.json',
            'pytorch_model.bin',
            'tokenizer.json',
            'tokenizer_config.json',
            'vocab.txt'
        ]
        
        for file_name in required_files:
            file_path = os.path.join(model_path, file_name)
            if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:
                logger.debug(f"æœ¬åœ°æ¨¡å‹ç¼ºå°‘æ–‡ä»¶: {file_path}")
                return False
        
        logger.debug(f"æœ¬åœ°æ¨¡å‹æ–‡ä»¶å®Œæ•´: {model_path}")
        return True

    def _load_fallback_model(self) -> bool:
        """åŠ è½½ç¦»çº¿fallbackæ¨¡å‹"""
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            import jieba
            import pickle
            import os
            
            logger.info("åˆå§‹åŒ–TF-IDF fallbackæ¨¡å‹...")
            
            # åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=768,  # è®¾ç½®å›ºå®šç»´åº¦
                stop_words=None,
                tokenizer=self._chinese_tokenizer,
                lowercase=True,
                ngram_range=(1, 2)
            )
            
            # åˆå§‹åŒ–çŠ¶æ€
            self.is_initialized = True
            self.model_name = "TF-IDF-Fallback"
            self.dimension = 768
            self.is_tfidf_model = True
            
            # åˆ›å»ºåˆå§‹è¯æ±‡è¡¨
            sample_texts = [
                "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£",
                "äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ ",
                "æ•°æ®ç§‘å­¦å’Œç®—æ³•",
                "ç¼–ç¨‹å’Œè½¯ä»¶å¼€å‘",
                "This is a test document",
                "Artificial intelligence and machine learning"
            ]
            
            # è®­ç»ƒTF-IDFæ¨¡å‹
            self.tfidf_vectorizer.fit(sample_texts)
            
            logger.info(f"TF-IDF fallbackæ¨¡å‹åŠ è½½æˆåŠŸï¼Œç»´åº¦: {self.dimension}")
            return True
            
        except Exception as e:
            logger.error(f"Fallbackæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

    def _chinese_tokenizer(self, text: str):
        """ä¸­æ–‡åˆ†è¯å™¨"""
        try:
            import jieba
            return list(jieba.cut(text))
        except ImportError:
            # å¦‚æœjiebaä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•åˆ†å‰²
            return text.split()

    def _try_load_model(self, model_name: str, model_config: Dict = None) -> bool:
        """å°è¯•åŠ è½½æŒ‡å®šæ¨¡å‹"""
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
            if model_name.startswith('./') or model_name.startswith('../'):
                import os
                if not os.path.exists(model_name):
                    logger.debug(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_name}")
                    return False
                logger.info(f"å°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹: {model_name}")
            else:
                logger.info(f"å°è¯•åŠ è½½åœ¨çº¿æ¨¡å‹: {model_name}")
            
            # æ–¹æ³•1: ä½¿ç”¨sentence-transformersï¼ˆæ¨èï¼‰
            if self._load_with_sentence_transformers(model_name):
                return True
            
            # æ–¹æ³•2: ä½¿ç”¨huggingface transformers
            if self._load_with_transformers(model_name):
                return True
            
            return False
            
        except Exception as e:
            logger.debug(f"åŠ è½½æ¨¡å‹ {model_name} å¤±è´¥: {e}")
            return False

    def _load_with_sentence_transformers(self, model_name: str) -> bool:
        """ä½¿ç”¨sentence-transformersåŠ è½½"""
        try:
            from sentence_transformers import SentenceTransformer
            
            # é…ç½®æ¨¡å‹åŠ è½½å‚æ•°
            device = str(self.device)
            trust_remote_code = True
            
            # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
            if os.path.exists(model_name):
                model_path = os.path.abspath(model_name)
                logger.info(f"åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}")
            else:
                model_path = model_name
                logger.info(f"åŠ è½½åœ¨çº¿æ¨¡å‹: {model_path}")
            
            # å°è¯•åŠ è½½æ¨¡å‹
            try:
                self.model = SentenceTransformer(
                    model_path, 
                    device=device,
                    trust_remote_code=trust_remote_code
                )
            except:
                # å¦‚æœsentence-transformerså¤±è´¥ï¼Œä½¿ç”¨transformersåº“
                return self._load_with_transformers_fallback(model_path)
            
            # æµ‹è¯•æ¨¡å‹å¹¶è·å–å®é™…ç»´åº¦
            test_embedding = self.model.encode(["æµ‹è¯•æ–‡æœ¬"], convert_to_numpy=True)
            self.dimension = test_embedding.shape[1]
            self.is_initialized = True
            
            logger.info(f"sentence-transformersåŠ è½½æˆåŠŸï¼Œå®é™…ç»´åº¦: {self.dimension}")
            return True
            
        except ImportError:
            logger.debug("sentence-transformersæœªå®‰è£…")
            return False
        except Exception as e:
            logger.debug(f"sentence-transformersåŠ è½½å¤±è´¥: {e}")
            return False

    def _load_with_transformers_fallback(self, model_path: str) -> bool:
        """ä½¿ç”¨transformersä½œä¸ºfallbackåŠ è½½æœ¬åœ°æ¨¡å‹"""
        try:
            from transformers import AutoTokenizer, AutoModel
            import torch
            
            logger.info("å°è¯•ä½¿ç”¨transformersåº“åŠ è½½æœ¬åœ°æ¨¡å‹...")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            self.model = AutoModel.from_pretrained(model_path, trust_remote_code=True)
            self.model.to(self.device)
            self.model.eval()
            
            # æµ‹è¯•è·å–ç»´åº¦
            with torch.no_grad():
                test_inputs = self.tokenizer("æµ‹è¯•æ–‡æœ¬", return_tensors="pt", padding=True, truncation=True)
                test_inputs = {k: v.to(self.device) for k, v in test_inputs.items()}
                outputs = self.model(**test_inputs)
                self.dimension = outputs.last_hidden_state.shape[-1]
            
            self.is_initialized = True
            self.is_transformers_model = True  # æ ‡è®°ä¸ºtransformersæ¨¡å‹
            logger.info(f"transformersåŠ è½½æˆåŠŸï¼Œç»´åº¦: {self.dimension}")
            return True
            
        except Exception as e:
            logger.debug(f"transformers fallbackåŠ è½½å¤±è´¥: {e}")
            return False

    def _load_with_transformers(self, model_name: str) -> bool:
        """ä½¿ç”¨transformersç›´æ¥åŠ è½½"""
        try:
            from transformers import AutoTokenizer, AutoModel
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
            self.model.to(self.device)
            self.model.eval()
            
            # æµ‹è¯•è·å–ç»´åº¦
            with torch.no_grad():
                test_inputs = self.tokenizer("æµ‹è¯•æ–‡æœ¬", return_tensors="pt", padding=True, truncation=True)
                test_inputs = {k: v.to(self.device) for k, v in test_inputs.items()}
                outputs = self.model(**test_inputs)
                self.dimension = outputs.last_hidden_state.shape[-1]
            
            self.is_initialized = True
            logger.info(f"transformersåŠ è½½æˆåŠŸï¼Œç»´åº¦: {self.dimension}")
            return True
            
        except Exception as e:
            logger.debug(f"transformersåŠ è½½å¤±è´¥: {e}")
            return False

    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡å‘é‡åŒ–æ–‡æœ¬"""
        if not self.is_initialized or not texts:
            return []
        
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºTF-IDF fallbackæ¨¡å‹
            if hasattr(self, 'is_tfidf_model') and self.is_tfidf_model:
                return self._embed_with_tfidf(texts)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºtransformersæ¨¡å‹
            if hasattr(self, 'is_transformers_model') and self.is_transformers_model:
                return self._embed_with_transformers(texts)
            
            # sentence-transformersæ–¹å¼
            if hasattr(self.model, 'encode'):
                embeddings = self.model.encode(
                    texts, 
                    convert_to_numpy=True,
                    batch_size=self.batch_size,
                    show_progress_bar=len(texts) > 100
                )
                return embeddings.tolist()
            else:
                # transformersæ–¹å¼
                return self._embed_with_transformers(texts)
                
        except Exception as e:
            logger.error(f"å‘é‡åŒ–å¤±è´¥: {e}")
            return []

    def _embed_with_tfidf(self, texts: List[str]) -> List[List[float]]:
        """ä½¿ç”¨TF-IDFè¿›è¡Œå‘é‡åŒ–"""
        try:
            # å°†æ–‡æœ¬è½¬æ¢ä¸ºTF-IDFå‘é‡
            tfidf_matrix = self.tfidf_vectorizer.transform(texts)
            
            # è½¬æ¢ä¸ºå¯†é›†çŸ©é˜µå¹¶å¡«å……åˆ°å›ºå®šç»´åº¦
            dense_matrix = tfidf_matrix.toarray()
            
            # å¦‚æœç»´åº¦ä¸è¶³ï¼Œç”¨é›¶å¡«å……
            if dense_matrix.shape[1] < self.dimension:
                padding = np.zeros((dense_matrix.shape[0], self.dimension - dense_matrix.shape[1]))
                dense_matrix = np.concatenate([dense_matrix, padding], axis=1)
            elif dense_matrix.shape[1] > self.dimension:
                # å¦‚æœç»´åº¦è¿‡å¤šï¼Œæˆªæ–­
                dense_matrix = dense_matrix[:, :self.dimension]
            
            return dense_matrix.tolist()
            
        except Exception as e:
            logger.error(f"TF-IDFå‘é‡åŒ–å¤±è´¥: {e}")
            # è¿”å›éšæœºå‘é‡ä½œä¸ºæœ€åçš„fallback
            return [[0.0] * self.dimension for _ in texts]

    def _embed_with_transformers(self, texts: List[str]) -> List[List[float]]:
        """ä½¿ç”¨transformersè¿›è¡Œå‘é‡åŒ–"""
        embeddings = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]
            
            with torch.no_grad():
                inputs = self.tokenizer(
                    batch_texts, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=512
                )
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                outputs = self.model(**inputs)
                # ä½¿ç”¨æ± åŒ–ç­–ç•¥è·å–å¥å­å‘é‡
                batch_embeddings = self._pool_embeddings(outputs.last_hidden_state, inputs['attention_mask'])
                embeddings.extend(batch_embeddings.cpu().numpy().tolist())
        
        return embeddings

    def _pool_embeddings(self, last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """æ± åŒ–ç­–ç•¥è·å–å¥å­çº§å‘é‡"""
        # ä½¿ç”¨mean pooling
        masked_embeddings = last_hidden_state * attention_mask.unsqueeze(-1)
        summed_embeddings = masked_embeddings.sum(dim=1)
        counts = attention_mask.sum(dim=1, keepdim=True).float()
        mean_embeddings = summed_embeddings / counts
        
        # L2æ ‡å‡†åŒ–
        return torch.nn.functional.normalize(mean_embeddings, p=2, dim=1)

    def embed_query(self, query: str) -> List[float]:
        """å‘é‡åŒ–å•ä¸ªæŸ¥è¯¢"""
        result = self.embed_texts([query])
        return result[0] if result else []

    def get_embedding_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        return self.dimension

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_name': self.model_name,
            'dimension': self.dimension,
            'device': str(self.device),
            'is_initialized': self.is_initialized,
            'batch_size': self.batch_size
        }

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.model is not None:
            del self.model
        if self.tokenizer is not None:
            del self.tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        logger.info("Embedding providerå·²æ¸…ç†")

    # å®ç°æŠ½è±¡æ¥å£æ–¹æ³•
    def encode(self, texts: Union[str, List[str]], batch_size: int = 32) -> np.ndarray:
        """å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡"""
        if isinstance(texts, str):
            texts = [texts]
        
        embeddings = self.embed_texts(texts)
        return np.array(embeddings)
    
    def get_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        return self.dimension
    
    def is_available(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        return self.is_initialized and self.model is not None
    
    def configure(self, **kwargs) -> None:
        """é…ç½®æ¨¡å‹å‚æ•°"""
        self.config.update(kwargs)
        self.batch_size = self.config.get('batch_size', 32)
        
        # å¦‚æœæŒ‡å®šäº†æ–°æ¨¡å‹ï¼Œé‡æ–°åŠ è½½
        if 'model_name' in kwargs:
            self._load_best_available_model()