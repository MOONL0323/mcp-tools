"""
ä½¿ç”¨å¼€æºembeddingæ¨¡åž‹çš„æœ¬åœ°å®žçŽ°
æ”¯æŒå¤šç§å¼€æºæ¨¡åž‹ï¼Œå®Œå…¨ç¦»çº¿è¿è¡Œ
"""
import os
import torch
import numpy as np
from typing import List, Union, Dict, Any
from loguru import logger

from interfaces.embedding_provider import EmbeddingProviderInterface


class LocalEmbeddingProvider(EmbeddingProviderInterface):
    """ä½¿ç”¨å¼€æºæ¨¡åž‹çš„æœ¬åœ°å‘é‡åŒ–æœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–å‘é‡åŒ–æœåŠ¡"""
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.dimension = 384  # é»˜è®¤ç»´åº¦ï¼Œä¼šæ ¹æ®å®žé™…æ¨¡åž‹è°ƒæ•´
        self.model_name = None
        self.is_initialized = False
        
        logger.info(f"æœ¬åœ°å‘é‡åŒ–æœåŠ¡åˆå§‹åŒ–ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        self._load_model()

    def _load_model(self):
        """åŠ è½½å½“å‰æœ€å…ˆè¿›çš„å¼€æºembeddingæ¨¡åž‹"""
        # å½“å‰æœ€é«˜çº§çš„å¼€æºæ¨¡åž‹ï¼ˆæŒ‰ä¼˜å…ˆçº§æŽ’åºï¼‰
        high_end_models = [
            (r"D:/huggingface_models/BAAI/bge-large-zh-v1.5", "BGEå¤§åž‹ä¸­æ–‡æ¨¡åž‹(1024ç»´)"),
            (r"D:/huggingface_models/BAAI/bge-large-en-v1.5", "BGEå¤§åž‹è‹±æ–‡æ¨¡åž‹(1024ç»´)"),
            (r"D:/huggingface_models/BAAI/bge-base-zh-v1.5", "BGEåŸºç¡€ä¸­æ–‡æ¨¡åž‹(768ç»´)"),
            (r"D:/huggingface_models/sentence-transformers/all-mpnet-base-v2", "MPNetå…¨èƒ½æ¨¡åž‹(768ç»´)"),
            (r"D:/huggingface_models/shibing624/text2vec-large-chinese", "Text2Vecå¤§åž‹ä¸­æ–‡æ¨¡åž‹(1024ç»´)"),
            (r"D:/huggingface_models/shibing624/text2vec-base-chinese", "Text2VecåŸºç¡€ä¸­æ–‡æ¨¡åž‹(768ç»´)"),
        ]
        
        # å°è¯•åŠ è½½æœ€é«˜çº§çš„å¯ç”¨æ¨¡åž‹
        for model_path, description in high_end_models:
            if os.path.exists(model_path):
                try:
                    logger.info(f"å°è¯•åŠ è½½{description}: {model_path}")
                    success = self._try_load_model(model_path)
                    if success:
                        self.model_name = model_path
                        logger.info(f"âœ… æˆåŠŸåŠ è½½{description}ï¼Œå‘é‡ç»´åº¦: {self.dimension}")
                        return
                except Exception as e:
                    logger.warning(f"æ— æ³•åŠ è½½ {model_path}: {e}")
        
        # å¦‚æžœæ‰€æœ‰é«˜çº§æ¨¡åž‹éƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨TF-IDF fallback
        logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•é«˜çº§embeddingæ¨¡åž‹ï¼Œä½¿ç”¨ç®€åŒ–çš„TF-IDFæ–¹æ³•")
        logger.info("ðŸ’¡ å»ºè®®ä¸‹è½½BGEæ¨¡åž‹ä»¥èŽ·å¾—æœ€ä½³æ•ˆæžœ: https://huggingface.co/BAAI/bge-large-zh-v1.5")
        self._init_simple_tfidf()
        self.is_initialized = True

    def _try_load_model(self, model_name: str) -> bool:
        """å°è¯•åŠ è½½æŒ‡å®šæ¨¡åž‹"""
        try:
            # æ–¹æ³•1: å°è¯•ä½¿ç”¨sentence-transformers
            if self._load_with_sentence_transformers(model_name):
                return True
            
            # æ–¹æ³•2: å°è¯•ä½¿ç”¨transformers
            if self._load_with_transformers(model_name):
                return True
            
            return False
            
        except Exception as e:
            logger.debug(f"åŠ è½½æ¨¡åž‹ {model_name} å¤±è´¥: {e}")
            return False

    def _load_with_sentence_transformers(self, model_name: str) -> bool:
        """ä½¿ç”¨sentence-transformersåŠ è½½æ¨¡åž‹"""
        try:
            from sentence_transformers import SentenceTransformer
            
            # å…ˆå°è¯•ä¸è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼ˆå…è®¸é¦–æ¬¡ä¸‹è½½ï¼‰
            try:
                self.model = SentenceTransformer(model_name, device=self.device)
            except Exception as e:
                # å¦‚æžœå¤±è´¥ï¼Œå°è¯•ç¦»çº¿æ¨¡å¼
                logger.warning(f"è”ç½‘åŠ è½½å¤±è´¥ï¼Œå°è¯•ç¦»çº¿æ¨¡å¼: {e}")
                os.environ['TRANSFORMERS_OFFLINE'] = '1'
                self.model = SentenceTransformer(model_name, device=self.device)
            
            # æµ‹è¯•æ¨¡åž‹å¹¶èŽ·å–ç»´åº¦
            test_embedding = self.model.encode(["test"], convert_to_numpy=True)
            self.dimension = test_embedding.shape[1]
            
            self.is_initialized = True
            logger.info(f"ä½¿ç”¨sentence-transformersåŠ è½½æˆåŠŸï¼Œç»´åº¦: {self.dimension}")
            return True
            
        except ImportError:
            logger.debug("sentence-transformersæœªå®‰è£…")
            return False
        except Exception as e:
            logger.debug(f"sentence-transformersåŠ è½½å¤±è´¥: {e}")
            return False

    def _load_with_transformers(self, model_name: str) -> bool:
        """ä½¿ç”¨transformersåŠ è½½æ¨¡åž‹"""
        try:
            from transformers import AutoTokenizer, AutoModel
            
            # è®¾ç½®ç¦»çº¿æ¨¡å¼
            os.environ['TRANSFORMERS_OFFLINE'] = '1'
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(model_name).to(self.device)
            self.model.eval()
            
            # æµ‹è¯•æ¨¡åž‹å¹¶èŽ·å–ç»´åº¦
            with torch.no_grad():
                test_inputs = self.tokenizer("test", return_tensors="pt", 
                                           padding=True, truncation=True, max_length=512)
                test_inputs = {k: v.to(self.device) for k, v in test_inputs.items()}
                outputs = self.model(**test_inputs)
                # ä½¿ç”¨[CLS]æ ‡è®°çš„è¾“å‡ºæˆ–æ± åŒ–è¾“å‡º
                if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
                    test_embedding = outputs.pooler_output
                else:
                    test_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token
                
                self.dimension = test_embedding.shape[1]
            
            self.is_initialized = True
            logger.info(f"ä½¿ç”¨transformersåŠ è½½æˆåŠŸï¼Œç»´åº¦: {self.dimension}")
            return True
            
        except ImportError:
            logger.debug("transformersæœªå®‰è£…")
            return False
        except Exception as e:
            logger.debug(f"transformersåŠ è½½å¤±è´¥: {e}")
            return False

    def _init_simple_tfidf(self):
        """åˆå§‹åŒ–ç®€å•çš„TF-IDFå‘é‡åŒ–æ–¹æ³•ï¼ˆå®Œå…¨ç¦»çº¿ï¼‰"""
        try:
            import jieba
            self.model = "simple_tfidf"
            self.dimension = 256  # å›ºå®šç»´åº¦
            self.vocab = {}  # è¯æ±‡è¡¨
            self.idf_weights = {}  # IDFæƒé‡
            logger.info(f"åˆå§‹åŒ–ç®€åŒ–TF-IDFæ–¹æ³•ï¼Œç»´åº¦: {self.dimension}")
        except ImportError:
            logger.error("éœ€è¦å®‰è£… jieba: pip install jieba")
            raise ImportError("éœ€è¦å®‰è£… jieba: pip install jieba")

    def encode(self, texts: Union[str, List[str]], batch_size: int = 32) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡"""
        if not self.is_initialized:
            raise RuntimeError("æ¨¡åž‹æœªåˆå§‹åŒ–")
        
        if isinstance(texts, str):
            texts = [texts]
        
        # ä½¿ç”¨ç®€å•TF-IDFæ–¹æ³•
        if self.model == "simple_tfidf":
            return self._encode_with_simple_tfidf(texts)
        
        # ä½¿ç”¨sentence-transformers
        if hasattr(self.model, 'encode'):
            try:
                embeddings = self.model.encode(texts, batch_size=batch_size, 
                                             convert_to_numpy=True, 
                                             show_progress_bar=False)
                return embeddings.astype(np.float32)
            except Exception as e:
                logger.error(f"sentence-transformersç¼–ç å¤±è´¥: {e}")
                raise
        
        # ä½¿ç”¨transformers
        elif self.tokenizer is not None:
            return self._encode_with_transformers(texts, batch_size)
        
        else:
            raise RuntimeError("æ— å¯ç”¨çš„ç¼–ç æ–¹æ³•")

    def _encode_with_transformers(self, texts: List[str], batch_size: int) -> np.ndarray:
        """ä½¿ç”¨transformersè¿›è¡Œç¼–ç """
        embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            # åˆ†è¯
            inputs = self.tokenizer(batch_texts, return_tensors="pt", 
                                  padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                
                # èŽ·å–å¥å­è¡¨ç¤º
                if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
                    batch_embeddings = outputs.pooler_output
                else:
                    # ä½¿ç”¨[CLS]æ ‡è®°æˆ–å¹³å‡æ± åŒ–
                    batch_embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] token
                
                embeddings.append(batch_embeddings.cpu().numpy())
        
        return np.vstack(embeddings).astype(np.float32)

    def _encode_with_simple_tfidf(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨ç®€å•çš„TF-IDFæ–¹æ³•ç¼–ç æ–‡æœ¬"""
        import jieba
        import math
        from collections import Counter
        
        embeddings = []
        
        for text in texts:
            # ä¸­æ–‡åˆ†è¯
            words = list(jieba.cut(text))
            
            # è®¡ç®—è¯é¢‘ (TF)
            word_count = Counter(words)
            total_words = len(words)
            
            # åˆ›å»ºå›ºå®šç»´åº¦çš„å‘é‡
            vector = np.zeros(self.dimension, dtype=np.float32)
            
            for word, count in word_count.items():
                # è®¡ç®—TF
                tf = count / total_words if total_words > 0 else 0
                
                # ä½¿ç”¨è¯æ±‡çš„å“ˆå¸Œå€¼æ˜ å°„åˆ°å‘é‡ä½ç½®
                hash_val = hash(word)
                idx = hash_val % self.dimension
                
                # ç®€å•ç´¯åŠ ï¼ˆå¯ä»¥è€ƒè™‘ä½¿ç”¨æ›´å¤æ‚çš„æ–¹æ³•ï¼‰
                vector[idx] += tf
            
            # å½’ä¸€åŒ–
            norm = np.linalg.norm(vector)
            if norm > 0:
                vector = vector / norm
            
            embeddings.append(vector)
        
        return np.array(embeddings, dtype=np.float32)

    def get_dimension(self) -> int:
        """èŽ·å–å‘é‡ç»´åº¦"""
        return self.dimension

    def get_model_info(self) -> Dict[str, Any]:
        """èŽ·å–æ¨¡åž‹ä¿¡æ¯"""
        return {
            "name": self.model_name or "Unknown",
            "type": "Transformer-based Embedding",
            "dimension": self.dimension,
            "device": self.device,
            "is_offline": True,
            "initialized": self.is_initialized
        }

    def is_available(self) -> bool:
        """æ£€æŸ¥æ¨¡åž‹æ˜¯å¦å¯ç”¨"""
        return self.is_initialized

    def configure(self, **kwargs) -> None:
        """é…ç½®æ¨¡åž‹å‚æ•°"""
        if 'device' in kwargs:
            self.device = kwargs['device']
            # åªæœ‰çœŸæ­£çš„PyTorchæ¨¡åž‹æ‰éœ€è¦ç§»åŠ¨åˆ°è®¾å¤‡
            if self.model is not None and hasattr(self.model, 'to') and self.model != "simple_tfidf":
                self.model = self.model.to(self.device)
            logger.info(f"è®¾ç½®è®¾å¤‡ä¸º: {self.device}")

    def similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        # ç¡®ä¿å‘é‡æ˜¯1ç»´çš„
        if vec1.ndim > 1:
            vec1 = vec1.flatten()
        if vec2.ndim > 1:
            vec2 = vec2.flatten()
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        similarity = dot_product / (norm1 * norm2)
        return float(similarity)